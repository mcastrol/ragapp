{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4953ff",
   "metadata": {},
   "source": [
    "Simple Pipeline that\n",
    "- read all the .txt from a folder\n",
    "- chunk data\n",
    "- apply embedding\n",
    "- save in Pgvector and qdrant preprod\n",
    "- retrieve data from pgvector\n",
    "- query a question using mistral \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330c1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from langchain.vectorstores.pgvector import DistanceStrategy\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeeeed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "local_path=os.getenv(\"LOCAL_PATH\")\n",
    "collection_name=os.getenv('COLLECTION_NAME')\n",
    "embedding_model=os.getenv('EMBEDDING_MODEL_NAME')\n",
    "chunk_size=int(os.getenv('CHUNK_SIZE'))\n",
    "chunk_overlap=int(os.getenv('CHUNK_OVERLAP'))\n",
    "pgddisconnection=os.getenv('PGDDISCONNECTION')\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\", \"\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b8d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Retrieval-Augmented Generation (RAG), and why is it useful?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "198efbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcastrol/.conda/envs/SimRAG/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs = {'device': 'cpu'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77deaaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/mcl_test\n"
     ]
    }
   ],
   "source": [
    "print(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef74d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(f'{local_path}', glob=\"./*.txt\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1387750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=['##'], chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb8a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/mcl_test/mcl_test.txt'}, page_content='# Building a Retrieval-Augmented Generation (RAG) System with LangChain\\n\\n## Introduction to RAG\\n\\nRetrieval-Augmented Generation (RAG) is a powerful method that combines retrieval of information with generative models. This approach is particularly effective in scenarios where the available data is too vast to be memorized by the model. Instead of relying solely on the model\\'s pre-trained knowledge, RAG leverages external data sources to provide more accurate and contextually relevant responses.\\n\\nLangChain is an excellent framework for building RAG systems. It provides tools to integrate various language models with external data sources, enabling the creation of dynamic and responsive applications.\\n\\n## Step-by-Step Guide to Building a RAG System with LangChain\\n\\n### 1. Setting Up LangChain\\n\\nTo begin, you\\'ll need to install LangChain and its dependencies. This can be done via pip:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\nOnce installed, you can start by importing the necessary modules.\\n\\n```python from langchain import OpenAI, VectorDBQA from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS ```\\n\\n### 2. Creating a Vector Store\\n\\nA core component of a RAG system is the vector store, which is used to index and retrieve documents based on their embeddings. LangChain provides integration with various vector stores, such as FAISS.\\n\\n```python # Create an embedding model embeddings = OpenAIEmbeddings()\\n\\n# Load documents and create embeddings documents = [\"Document 1 content...\", \"Document 2 content...\", \"Document 3 content...\"] document_embeddings = [embeddings.embed(doc) for doc in documents]\\n\\n# Create a FAISS vector store vector_store = FAISS(embeddings.dimension) vector_store.add_documents(documents, document_embeddings) ```\\n\\n### 3. Implementing the Retrieval Component\\n\\nWith the vector store in place, the next step is to implement the retrieval component. This component will search for relevant documents based on a user\\'s query.\\n\\n```python\\n\\ndef retrieve(query):\\n\\nquery_embedding = embeddings.embed(query)\\n\\nresults = vector_store.search(query_embedding, k=5)\\n\\nreturn results\\n\\n```\\n\\n### 4. Generating Responses\\n\\nThe final piece of the puzzle is generating responses using the retrieved documents. LangChain\\'s `VectorDBQA` class can be used for this purpose.\\n\\n```python\\n\\nqa = VectorDBQA(llm=OpenAI(), vector_store=vector_store)\\n\\ndef generate_response(query):\\n\\nreturn qa.run(query)\\n\\n```\\n\\n### 5. Testing the RAG System\\n\\nNow that the system is set up, you can test it with various queries. The RAG system will first retrieve the most relevant documents and then generate a response using those documents as context.\\n\\n```python query = \"What is LangChain?\" response = generate_response(query) print(response) ```\\n\\n### Conclusion\\n\\nBuilding a RAG system with LangChain is a straightforward process that involves setting up a vector store, implementing a retrieval component, and integrating it with a language model for response generation. This system is particularly useful for applications that require accurate and context-aware information retrieval from large datasets.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ebe4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ed17af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/mcl_test/mcl_test.txt'}, page_content=\"# Building a Retrieval-Augmented Generation (RAG) System with LangChain\\n\\n## Introduction to RAG\\n\\nRetrieval-Augmented Generation (RAG) is a powerful method that combines retrieval of information with generative models. This approach is particularly effective in scenarios where the available data is too vast to be memorized by the model. Instead of relying solely on the model's pre-trained knowledge, RAG leverages external data sources to provide more accurate and contextually relevant responses.\\n\\nLangChain is an excellent framework for building RAG systems. It provides tools to integrate various language models with external data sources, enabling the creation of dynamic and responsive applications.\\n\\n## Step-by-Step Guide to Building a RAG System with LangChain\"),\n",
       " Document(metadata={'source': 'data/mcl_test/mcl_test.txt'}, page_content=\"### 1. Setting Up LangChain\\n\\nTo begin, you'll need to install LangChain and its dependencies. This can be done via pip:\\n\\n```bash\\n\\npip install langchain\\n\\n```\\n\\nOnce installed, you can start by importing the necessary modules.\\n\\n```python from langchain import OpenAI, VectorDBQA from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS ```\"),\n",
       " Document(metadata={'source': 'data/mcl_test/mcl_test.txt'}, page_content='### 2. Creating a Vector Store\\n\\nA core component of a RAG system is the vector store, which is used to index and retrieve documents based on their embeddings. LangChain provides integration with various vector stores, such as FAISS.\\n\\n```python # Create an embedding model embeddings = OpenAIEmbeddings()\\n\\n# Load documents and create embeddings documents = [\"Document 1 content...\", \"Document 2 content...\", \"Document 3 content...\"] document_embeddings = [embeddings.embed(doc) for doc in documents]\\n\\n# Create a FAISS vector store vector_store = FAISS(embeddings.dimension) vector_store.add_documents(documents, document_embeddings) ```\\n\\n### 3. Implementing the Retrieval Component\\n\\nWith the vector store in place, the next step is to implement the retrieval component. This component will search for relevant documents based on a user\\'s query.\\n\\n```python\\n\\ndef retrieve(query):\\n\\nquery_embedding = embeddings.embed(query)\\n\\nresults = vector_store.search(query_embedding, k=5)\\n\\nreturn results\\n\\n```')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "505e1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PGVector.from_documents(\n",
    "    documents= texts,\n",
    "    embedding = embeddings,\n",
    "    collection_name= collection_name,\n",
    "    distance_strategy = DistanceStrategy.COSINE,\n",
    "    pre_delete_collection = True,\n",
    "    connection=pgddisconnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "714ecee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.qdrant.Qdrant at 0x7f6d0cc1ec90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Qdrant.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key,\n",
    "    port=None,\n",
    "    collection_name=collection_name,\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91906f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PGVector(\n",
    "        connection=pgddisconnection, \n",
    "        collection_name=collection_name, \n",
    "        embeddings=embeddings,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vector_store.similarity_search(question, k=2)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for which we want to find semantically similar documents\n",
    "\n",
    "\n",
    "#Fetch the k=2 most similar documents\n",
    "docs =  db.similarity_search(question, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1495f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '\\n'.join([x.page_content for x in docs])\n",
    "\n",
    "\n",
    "prompt = f\"\"\"[INST]You are a helpful chatbot that can answer questions based on the provided context. \n",
    "You need not make use of the entire context provided to you.\n",
    "Try to interpret the question. If it is a general question asking for definitions, you can rephrase the content without changing the meaning of it.\n",
    "If the asked question demands steps or process or procedure, do not change the content and stick to the original form as possible. Also if context has Red Hat specific knowledge add that in answer.\n",
    "Also provide the source from which you took the answer under source: tag\n",
    "\n",
    "Context: {context} [\\INST]\n",
    "Question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ddis-mistral-7b.apps.int.stc.ai.preprod.us-east-1.aws.paas.redhat.com/v1/chat/completions'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"mistral-7b\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data), verify=False, timeout=30)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e563ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json()['choices'][0]['message']['content']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing llm variables\n",
    "# openai_api_key = constants.OPEN_AI_KEY\n",
    "# openai_api_base = \"{llm_url}/v1\".format(llm_url=constants.LLM_URL)\n",
    "# Fetch model information\n",
    "try:\n",
    "    response = requests.get(\"https://ddis-mistral-7b.apps.int.stc.ai.preprod.us-east-1.aws.paas.redhat.com/v1/models\")\n",
    "    response.raise_for_status()\n",
    "    model = response.json()[\"data\"][0][\"id\"]\n",
    "    print(f\"Model ID: {model}\")\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Failed to fetch model information: {e}\")\n",
    "    raise\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1aaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"https://ddis-mistral-7b.apps.int.stc.ai.preprod.us-east-1.aws.paas.redhat.com/v1\",\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    stream=True,\n",
    "    user='user_identifier',\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "        response += chunk.choices[0].delta.content\n",
    "response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
